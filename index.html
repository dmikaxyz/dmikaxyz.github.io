<!DOCTYPE HTML>
<html lang="en">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Daniel Mika</title>
		<meta name="author" content="Daniel Mika">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
		<link rel="stylesheet" type="text/css" href="stylesheet.css">
	</head>

	<body>
		<div class="container">
			<div class="content">
				<div class="header">
					<div class="header-text">
						<p class="name">Daniel Mika</p>
						<p>I'm a third-year student at the University of Pennsylvania in the <a href="https://fisher.wharton.upenn.edu/" target="_blank">M&T program</a> pursuing <a href="https://ai.seas.upenn.edu/" target="_blank">B.S.E in AI</a> and B.S. in Economics, and concurrently a <a href="https://www.grasp.upenn.edu/academics/masters-degree-program/" target="_blank">Masters in Robotics</a> at Penn Engineering.</p>
						<p>My main interest is computer vision with a focus on 3D/4D reconstruction and generation from sparse input, generative video models, camera control in video generation, and 4D scene generation. I am a part of the Penn Computer Graphics Lab working with my advisor <a href="https://lingjie0206.github.io/" target="_blank">Dr. Lingjie Liu</a> and also collaborating with researchers from <a href="https://www.grasp.upenn.edu/" target="_blank">Penn GRASP Lab</a>. My long-term goal is to contribute to the development of foundational vision models that can understand, simulate, and render the 4D-quarable representation of our world</p>
						<p>Prior to beginning my studies at Penn I completed engineering school in Poland (SWE) and worked on an AI startup called <a href="https://getdressed.me/en" target="_blank">getdressed</a> as CTO for 4.5 years. Beyond that, I worked as a data scientist and as an independent AI applied research consultant on multiple commercial projects between 2017-2022.</p>
						<div class="social-links">
							<a href="mailto:dmika@seas.upenn.edu" target="_blank">Email</a>
							<a href="data/DanielMika-CV.pdf" target="_blank">CV</a>
							<a href="https://www.linkedin.com/in/daniel-mika/" target="_blank">LinkedIn</a>
							<a href="https://twitter.com/DanielMika_" target="_blank">Twitter</a>
						</div>
					</div>
					<div class="header-image">
						<a href="images/DanielMika3.jpg" target="_blank">
							<img alt="profile photo" src="images/DanielMika3.jpg">
						</a>
					</div>
				</div>

				<div class="section">
					<h2>Research</h2>
					<h3>Current projects</h3>
					<p>My current work is focused on training 4D models using video data. The goal is to enable 4D consistent and controllable view genertion based on sparse input. If you are interested in collaborating on any of these projects, please reach out!</p>
				</div>

				<div class="project">
					<div class="project-media">
						<a href="images/4DNVS.png" target="_blank">
							<img src="images/4DNVS.png" alt="4D Project">
						</a>
					</div>
					<div class="project-content">
						<h3 class="project-title">Generative 4D NVS in time and space from sparse input using latent 4D gaussians and video diffusion</h3>
						<p>We finetune open source video diffusion models such as <a href="https://www.genmo.ai/blog" target="_blank">Mochi</a> or <a href="https://github.com/hpcaitech/Open-Sora" target="_blank">OpenSora</a> by using a multiview ViT encoder to predict 4D gaussian latents, enabling novel view synthesis with camera and time control through feature map conditioning. Training enforces 4D consistency in latent space through targeted pretraining tasks and enables recursive inference capabilities.</p>
						<div class="show-more-container">
							<button onclick="toggleContent('4dnvs-details')" class="show-more-btn">
								<span class="show-more-text">Show More</span>
								<span class="arrow">▼</span>
							</button>
							<div id="4dnvs-details" class="more-content">
								<p>The goal of the project is to finetune open-source video diffusion model such as <a href="https://www.genmo.ai/blog" target="_blank">Mochi</a> or <a href="https://github.com/hpcaitech/Open-Sora" target="_blank">OpenSora</a> to enable the conditional generation of novel views of the scene controlled both with relative camera and time. I proposed encoder-decoder architecture where we first train a multiview ViT (encoder) to predict a set of latent 4D Gaussians that describe the scene, then render them into 2D feature maps at the requested time/camera position. These feature maps together with the latents of known views are used as a conditioning signal for the finetuned video diffusion model (decoder) to perform denoising to clean latent. They are decoded to images at particular times and camera positions for NVS.</p>
								<p>This research direction was originally inspired by <a href="https://reconfusion.github.io/" target="_blank">ReconFusion</a>, <a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/" target="_blank">latentSplatting</a>, and later <a href="https://cat3d.github.io/" target="_blank">CAT3D</a>. Currently, other works have been proposed such as <a href="https://4d-diffusion.github.io/" target="_blank">4DiM</a>, <a href="https://cat-4d.github.io/" target="_blank">CAT4D</a>, <a href="https://snap-research.github.io/vd3d/" target="_blank">VD3D</a>, <a href="https://snap-research.github.io/ac3d/" target="_blank">AC3D</a>, and more that focus on this task approaching it from different directions.</p>
								<p>The key idea here is to apply 4D consistency in the latent space instead of the image space. We can learn a lot about the scene dynamics and world state distribution just by aligning the predictions of our multiview ViT encoder – we train it to predict the latents that match the original VAE encoder of the video diffusion model per input frame. A mix of pretraining tasks can be set up to achieve this. For example, we can calculate the loss between the VAE output for an image and the distribution rendered from 4D splats at that position. We can do that for a full image that was in the input to the multive ViT or we can drop some image tokens from the context for multiview ViT and ask it to predict a distribution to fill the gaps. Moreover, we can query the 4D field at positions that are not in the ViT context but we have in the training data (for example some other frame in the video that was not used as a condition). This enables strong 4D consistency pretraining just using the Encoder of VAE and our multiview ViT without the need for pushing gradients through the whole architecture of the video diffusion model at the beginning. Then at the second stage, we can train the whole model together. Thanks to the flexibility of rendering at 4D we can set up a flexible task such as generating views at different relative timesteps and different camera positions.</p>
								<p>The encoder will be implemented with NaViT-like architecture using Patch'n'Pack and token dropping and I plan to use <a href="https://arxiv.org/abs/2304.07193" target="_blank">DinoV2</a> feature maps together with RGB for each input view. The architecture can also be easily expanded to add a longer context length for 4D latent gaussians, this will enable recursive inference of the architecture while we grow/edit the latent Gaussian field that describes the scene, which will effectively add a memory mechanism to the 4D NVS model.</p>
								<p>I am looking for more collaborators to work on this ambitious project together! Also if you happen to have an available compute and are willing to share, please let me know. :)</p>
							</div>
						</div>
					</div>
				</div>

				<div class="project">
					<div class="project-content">
						<h3 class="project-title">Active training data selection for generative video model training</h3>
						<p>I am working on a self-supervised method that dynamically selects the most informative parts of video data during training. Current methods use uniform temporal subsampling, but the information density in videos varies significantly across time and space, making this suboptimal for training efficiency.</p>
						<div class="show-more-container">
							<button onclick="toggleContent('active-details')" class="show-more-btn">
								<span class="show-more-text">Show More</span>
								<span class="arrow">▼</span>
							</button>
							<div id="active-details" class="more-content">
								<p>The closest prior work exploring this problem is <a href="https://arxiv.org/abs/2406.17711" target="_blank">JEST (Data curation via joint example selection)</a>, which proposed using distilled smaller models to compose training batches from larger super-batches for image-language contrastive learning. However, they didn't explore video data or the possibility of selective patch-dropping, which presents unique opportunities and challenges, especially for generative video models.</p>
								<p>I'm exploring several promising approaches. One of the ideas is developing a hypernetwork that could predict which data the larger model should train on. This concept shares some similarities with multi-stage point sampling in NeRF methods, where we initially sample points on the ray uniformly, then predict the density with a lightweight network which guides more focused sampling in high-density regions which is then used as input for the main NeRF network. The key challenge here is ensuring the computational overhead of the selection process is outweighed by the training efficiency gains.</p>
								<p>The problem becomes particularly interesting in masked token prediction or sequence prediction tasks. Here, the curriculum model needs to make multiple decisions: (1) which patches of data to include in training, (2) what should serve as input/context, and (3) what should be designated as prediction targets for the main model.</p>
								<p>While reinforcement learning might be necessary for optimal policy learning, I'm also investigating simpler, computationally efficient heuristics that could bootstrap the selection process. For instance, using compression as an initial proxy for information density could provide a practical starting point. Key research questions include: (1) balancing selection overhead against training efficiency gains, (2) designing architectures that allow for efficient joint training of selector and main models, (3) developing effective reward signals for the selection process, and (4) determining whether simple heuristics can approach the performance of learned selection strategies.</p>
								<p>I believe this research could lead to fundamental improvements in how we approach training data selection for large-scale video models. The potential for joint training of selector and main models is particularly exciting, as it could enable adaptive, content-aware training that significantly improves training efficiency and yields significantly better model performance.</p>
							</div>
						</div>
					</div>
				</div>

				<div class="section">
					<h3>Previous Projects</h3>
				</div>

				<div class="project">
					<div class="project-content">
						<h3 class="project-title">AttentionNGP: Implicit Attention-based regularization for Multiresolution Hash Encodings</h3>
						<p>Adding a local attention mechanism into the process of training hash grid embeddings to collect information from multiple samples in proximity and distribute gradient to adjacent embeddings in an unbiased, data-dependent manner.</p>
						<div class="show-more-container">
							<button onclick="toggleContent('attention-details')" class="show-more-btn">
								<span class="show-more-text">Show More</span>
								<span class="arrow">▼</span>
							</button>
							<div id="attention-details" class="more-content">
								<p>The research project I pursued in Jan 2024. The idea was inspired by very successful 3D reconstruction models such as <a href="https://jonbarron.info/mipnerf360/" target="_blank">Mip-NeRF 360</a>, <a href="https://jonbarron.info/zipnerf/" target="_blank">Zip-NeRF</a>, and <a href="https://research.nvidia.com/labs/dir/neuralangelo/" target="_blank">NeuralAngelo</a>. The common idea of these methods was sampling points around the ray using different principled heuristics allowing the gradient to flow to adjacent cubes in the iNGP grid in the reconstruction process. My idea was to add a local attention mechanism into the process of training hash grid embeddings in order to collect information from multiple samples in proximity and distribute gradient to adjacent embeddings in an unbiased, data-dependent manner. This mechanism can be thought of as effectively "adjusting" the borders of the hash grid to more effectively fit the density of distribution and complexity of textures in modeled scenes throughout the training process. This was done by initializing 3 iNGP hash grids Q, K, V, randomly sampling points around the ray, and calculating the attention score for each point using vectors from all 3 grids. By adjusting embeddings the model could steer focus to a particular part of the space around the ray (by increasing alignment between Q and K vectors).</p>
								<p>The goal was to enable faster and better convergence by allowing the model to steer the gradient flow. In the end, the model achieved a result very close to Zip-NeRF using its original testing setup (only 0.2 PSNR lower on average) but fell short of beating this very strong baseline.</p>
							</div>
						</div>
					</div>
				</div>

				<div class="section">
					<h2>Applied Research</h2>
					<p>Selected commercial applied research projects I worked on in the past in Computer Vision and Recommendation Systems. Note that some information is omitted due to NDAs.</p>
				</div>

				<div class="project">
					<div class="project-media">
						<a href="images/getdressed1.png" target="_blank">
							<img src='images/getdressed1.png' alt="GetDressed">
						</a>
					</div>
					<div class="project-content">
						<h3 class="project-title">Fashion Style Space Learning: Creating AI-Powered Outfit Recommendations Through Multimodal Contrastive Learning</h3>
						<p>2018-2020 | <a href="https://www.getdressed.me/en" target="_blank">GetDressed</a></p>
						<p>Daniel Mika, Szymon Kurowski, Franciszek Stachura, Bartlomiej Hada, Eryk Dziedzic</p>
						<p>We designed a model to map fashion products into a unified style embedding space where stylistically matching items have high cosine similarity. The system learns from multiple product images, text descriptions, and categorical metadata to generate outfit recommendations and complete partial outfits.</p>
						<div class="show-more-container">
							<button onclick="toggleContent('fashion-details')" class="show-more-btn">
								<span class="show-more-text">Show More</span>
								<span class="arrow">▼</span>
							</button>
							<div id="fashion-details" class="more-content">
								<p>The core architecture uses metric learning to encode fashion items such that stylistically compatible products are mapped close together in the embedding space. Our training process involved iterative feedback from 20 fashion stylists in a setup similar to modern RLHF (Reinforcement Learning from Human Feedback) approaches.</p>
								<p>For each product, the model processes multiple image views, product descriptions, and category information through dedicated encoders. The multimodal representations are then projected into a shared embedding space using contrastive learning objectives. We developed category-based heuristics to ensure practical outfit combinations while preserving learned style relationships.</p>
								<p>The system supports two main applications: completing partial outfits by suggesting complementary items, and generating full outfit recommendations from style descriptors like "elegant" or "sports". The trained embeddings enable efficient retrieval through cosine similarity, making the system practical for large-scale fashion catalogs.</p>
							</div>
						</div>
					</div>
				</div>

				<div class="project">
					<div class="project-media">
            <a href="images/getdressed2.png" target="_blank">
              <img src='images/getdressed2.png' alt="Fashion2 Before">
            </a>
					</div>
					<div class="project-content">
						<h3 class="project-title">Style-aware Fashion Product Recommendation System for E-commerce</h3>
						<p>2020-2021 | <a href="https://www.getdressed.me/en" target="_blank">GetDressed</a></p>
						<p>Daniel Mika, Szymon Kurowski, Franciszek Stachura, Bartlomiej Hada, Eryk Dziedzic</p>
						<p>We developed a real-time fashion recommendation system using our pre-trained style network as a feature encoder, combined with graph neural networks to process user behavior during e-commerce sessions.</p>
						<div class="show-more-container">
							<button onclick="toggleContent('fashion2-details')" class="show-more-btn">
								<span class="show-more-text">Show More</span>
								<span class="arrow">▼</span>
							</button>
							<div id="fashion2-details" class="more-content">
								<p>Our approach builds upon and adapts <a href="https://snap.stanford.edu/graphsage/" target="_blank">GraphSAGE</a> architecture (similar to how Pinterest developed its <a href="https://arxiv.org/abs/1806.01973" target="_blank">PinSAGE</a> implementation) with specific optimizations for the the fashion domain. The system operates in two phases: first, our pre-trained multimodal style network processes the e-commerce catalog, extracting style features from product images, descriptions, and metadata. These features combined with historical e-commerce interaction data (co-browsing, co-purchase patterns between items, and shopping sessions) are used to initialize a pre-trained Graph Convolutional Network (GCN).</p>
								<p>In the second phase, as users interact with the platform, their actions generate events that update the GCN's state in real-time for their session. This enables dynamic recommendation features like outfit completion, complimentary items recommendations, and occasion-specific suggestions while taking into account other e-commerce inputs and constraints such as price ranges and shopping cart state.</p>
								<p>The key technical challenge was scaling to millions of products while maintaining real-time response capabilities. Our architecture allowed us to perform expensive batch-processing for feature extraction offline while using light-weight real-time GraphSAGE operations for handling session-based updates and inference. This system demonstrated how pre-trained vision-based domain-specific networks can improve dynamic recommendation systems.</p>
							</div>
						</div>
					</div>
				</div>

				<div class="project">
					<div class="project-content">
						<h3 class="project-title">Vision-based Quality Assurance System for Window Glass Testing</h3>
						<p>2021 | Under NDA for the German window glass manufacturer</p>
						<p>We developed an automated quality control system using computer vision and structured light to detect defects in window glass panels, reducing manual inspection needs by 90% while maintaining high accuracy.</p>
						<div class="show-more-container">
							<button onclick="toggleContent('glass-details')" class="show-more-btn">
								<span class="show-more-text">Show More</span>
								<span class="arrow">▼</span>
							</button>
							<div id="glass-details" class="more-content">
								<p>The project involved designing a multi-camera and laser setup for capturing high-quality inspection data. We collected a comprehensive dataset during parallel manual inspection processes, then fine-tuned a DINO (self-supervised vision transformer) model to classify defective glass planes based on multiple views and laser configurations.</p>
								<p>The production system processes images from different camera views and lighting conditions, providing automated decisions about glass panel quality. The deployed solution successfully automated most of the quality control process while maintaining rigorous safety standards required in glass manufacturing, with defective panels being flagged for targeted manual inspection.</p>
							</div>
						</div>
					</div>
				</div>

				<div class="project">
					<div class="project-content">
						<h3 class="project-title">Temporal-aware Graph Neural Network for Product Recommendations</h3>
						<p>2020 | Under NDA for the UK startup</p>
						<p>Designed and implemented a recommendation system that combines real-time user interaction data with product content embeddings in a Graph Neural Network architecture.</p>
						<div class="show-more-container">
							<button onclick="toggleContent('temporal-details')" class="show-more-btn">
								<span class="show-more-text">Show More</span>
								<span class="arrow">▼</span>
							</button>
							<div id="temporal-details" class="more-content">
								<p>The system processes temporal event sequences from user sessions alongside rich product content representations in a unified GNN framework. We developed specialized temporal encoding methods to capture user behavior patterns and their evolution over time considering patterns of behavior that changed continuously while maintaining real-time recommendation capabilities.</p>
							</div>
						</div>
					</div>
				</div>

				<div class="footer">
					<p>Website adapted from <a href="https://github.com/jonbarron/jonbarron_website" target="_blank">Jon Barron</a>.</p>
				</div>
			</div>
		</div>

		<script type="text/javascript">

			function toggleContent(elementId) {
				const content = document.getElementById(elementId);
				const btn = content.previousElementSibling;
				const showMoreText = btn.querySelector('.show-more-text');
				const isHidden = window.getComputedStyle(content).display === 'none';
				
				content.style.display = isHidden ? 'block' : 'none';
				showMoreText.textContent = isHidden ? 'Show Less' : 'Show More';
				btn.classList.toggle('active');
				
				// Initialize all content sections if not already done
				if (!content.style.display) {
					content.style.display = 'none';
				}
			}

			// Add this to initialize all sections on page load
			document.addEventListener('DOMContentLoaded', function() {
				const allMoreContent = document.querySelectorAll('.more-content');
				allMoreContent.forEach(content => {
					content.style.display = 'none';
				});
			});
		</script>
	</body>
</html>
